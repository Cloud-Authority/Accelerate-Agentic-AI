{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Semantic Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lab Introduction\n",
    "\n",
    "In this lab, we’ll explore Microsoft’s **Semantic Kernel**, a lightweight, open-source SDK that effortlessly integrates large language models (LLMs) into Python applications. You’ll configure the Semantic Kernel environment, define semantic and native functions, and build a basic AI agent with chat capabilities.\n",
    "\n",
    "### Why Use Semantic Kernel?\n",
    "\n",
    "* **Middleware for LLMs**: It manages prompt generation and LLM interaction, automatically calling registered functions when needed.\n",
    "* **Modular and Extensible**: Easily plug in your own code via native plugins or even OpenAPI specs.\n",
    "* **Cross‑language Support**: Though we’re using Python here, Semantic Kernel also supports C# and Java .\n",
    "\n",
    "### What You’ll Achieve:\n",
    "\n",
    "* Set up the Kernel and connect to Azure/OpenAI LLM backends\n",
    "* Create and invoke **semantic functions** (using prompt templates)\n",
    "* Build **native plugins** to extend functionality via real code\n",
    "* Chain prompts and functions into coherent workflows\n",
    "* Optionally, explore PLM agents, memory, vector store integration, and grounding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Import Semantic Kernel SDK from pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if using a virtual environment, do not run this cell\n",
    "# %pip install -qU semantic-kernel\n",
    "%pip install --upgrade semantic-kernel openai\n",
    "from semantic_kernel import __version__\n",
    "\n",
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial configuration for the notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - bookstruck1\\Azure\\Microsoft Workshops\\AgenticAI\\Labs\n"
     ]
    }
   ],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)\n",
    "print(grandparent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GLOBAL_LLM_SERVICE=\"AzureOpenAI\"\n",
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\n",
    "AZURE_OPENAI_TEXT_DEPLOYMENT_NAME=\"gpt-4o-mini\"\n",
    "AZURE_OPENAI_API_VERSION=\"2024-10-21\"\n",
    "\n",
    "os.environ[\"GLOBAL_LLM_SERVICE\"] = GLOBAL_LLM_SERVICE\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\n",
    "os.environ[\"AZURE_OPENAI_TEXT_DEPLOYMENT_NAME\"] = AZURE_OPENAI_TEXT_DEPLOYMENT_NAME\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = AZURE_OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: azureopenai\n"
     ]
    }
   ],
   "source": [
    "selectedService = (\n",
    "    \"azureopenai\"\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Semantic Function\n",
    "\n",
    "Let's load a Plugin and run a semantic function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"./plugins\", plugin_name=\"Summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "summary_function = plugin[\"summarize\"]\n",
    "\n",
    "summary = await kernel.invoke(\n",
    "    summary_function,\n",
    "    KernelArguments(input=\"\"\"\n",
    "Azure Cognitive Search is a cloud-native search service that unlocks powerful full-text search, AI-powered cognitive skills (like OCR, key phrase extraction, and language detection), and integrated semantic ranking. It supports indexing from various data sources (Blob, SQL, Cosmos DB) with minimal setup and is ideal for building search-driven apps and knowledge agents.\"\"\"),\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run a prompt plugins from file\n",
    "\n",
    "Now that you understand the basics of the Kernel, let's explore how you can execute Prompt Plugins and Prompt Functions that are saved on disk.\n",
    "\n",
    "A Prompt Plugin consists of multiple Semantic Functions, each defined using natural language in a text file.\n",
    "\n",
    "For example, here is the Summary function from the Summarizer plugin:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn what prompts are and how to write them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SUMMARIZE THE TEXT BELOW IN **EXACTLY THREE** BULLET POINTS.\n",
    "\n",
    "RULES\n",
    "- EACH BULLET ≤ 25 WORDS\n",
    "- NO FULL-SENTENCE QUOTES FROM THE SOURCE\n",
    "- KEEP NEUTRAL, FACTUAL TONE (NO OPINIONS OR NEW INFO)\n",
    "\n",
    "+++++\n",
    "{{$input}}\n",
    "+++++\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the special **`{{$input}}`** token, which is a variable that is automatically passed when invoking the function, commonly referred to as a \"function parameter\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same plugin folder you'll see a second config.json file. The file is optional, and is used to set parameters for large language models\n",
    "\n",
    "```\n",
    "{\n",
    "  \"schema\": 1,\n",
    "  \"description\": \"Summarizes text into 3 bullet points\",\n",
    "  \"type\": \"completion\",\n",
    "  \"completion\": {\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8\n",
    "  },\n",
    "  \"input\": {\n",
    "    \"parameters\": [\n",
    "      {\n",
    "        \"name\": \"input\",\n",
    "        \"description\": \"The text to summarize\",\n",
    "        \"default\": \"\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a prompt function defined by these files, this is how to load and use a file based prompt function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the plugin and all its functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"./plugins\", plugin_name=\"Summarizer\")\n",
    "\n",
    "summarizeFunction = plugin[\"summarize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the plugin functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Azure Cognitive Search is a cloud-native service for full-text search and AI cognitive skills.  \n",
      "- It supports indexing from multiple data sources, including Blob, SQL, and Cosmos DB.  \n",
      "- The service facilitates the development of search-driven applications and knowledge agents.  \n"
     ]
    }
   ],
   "source": [
    "result = await kernel.invoke(summarizeFunction, input=\"\"\"\n",
    "Azure Cognitive Search is a cloud-native search service that unlocks powerful full-text search, AI-powered cognitive skills (like OCR, key phrase extraction, and language detection), and integrated semantic ranking. It supports indexing from various data sources (Blob, SQL, Cosmos DB) with minimal setup and is ideal for building search-driven apps and knowledge agents.\"\"\", style=\"silly\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Prompt Functions Inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we demonstrated how to define a semantic function using a prompt template saved in a file.\n",
    "\n",
    "Now, we'll explore how to define semantic functions directly within your Python code using Semantic Kernel. This approach is helpful when:\n",
    "\n",
    "- You need to generate prompts dynamically based on runtime logic\n",
    "- You prefer editing prompts within Python rather than separate TXT files\n",
    "- You want to quickly prototype or build demos, as in this section\n",
    "\n",
    "Prompt templates use the SK template language, which lets you reference variables and functions.\n",
    "\n",
    "For now, we'll focus on the `{{$input}}` variable, with more advanced templates to come.\n",
    "\n",
    "Most semantic function prompts include `{{$input}}`, which is the standard way to pass content from context variables into your prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "        endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a prompt to create a semantic function used to summarize content\n",
    "\n",
    "The function will take in input the text to summarize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"{{$input}}\n",
    "Summarize the content above.\n",
    "\"\"\"\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "summarize = kernel.add_function(\n",
    "    function_name=\"summarizeFunc\",\n",
    "    plugin_name=\"summarizePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some content to summarize, here's an extract about Bill Gates, taken from Wikipedia [source](https://en.wikipedia.org/wiki/Bill_Gates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Demo (Bill Gates)\n",
    "William Henry Gates III (born October 28, 1955) is an American businessman and philanthropist. A pioneer of the microcomputer revolution of the 1970s and 1980s, he co-founded the software company Microsoft in 1975 with his childhood friend Paul Allen. Following the company's 1986 initial public offering (IPO), Gates became a billionaire in 1987—then the youngest ever, at age 31. Forbes magazine ranked him as the world's wealthiest person for 18 out of 24 years between 1995 and 2017, including 13 years consecutively from 1995 to 2007. He became the first centibillionaire in 1999, when his net worth briefly surpassed $100 billion. According to Forbes, as of May 2025, his net worth stood at US$115.1 billion, making him the thirteenth-richest individual in the world.\n",
    "\n",
    "Born and raised in Seattle, Washington, Gates was privately educated at Lakeside School, where he befriended Allen and developed his computing interests. In 1973, he enrolled at Harvard College, where he took classes including Math 55 and graduate level computer science courses, but he dropped out in 1975 to co-found and lead Microsoft. He served as its CEO for the next 25 years and also became president and chairman of the board when the company incorporated in 1981. Succeeded as CEO by Steve Ballmer in 2000, he transitioned to chief software architect, a position he held until 2008. He stepped down as chairman of the board in 2014 and became technology adviser to CEO Satya Nadella and other Microsoft leaders, a position he still holds. He resigned from the board in 2020.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run the summary function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Henry Gates III, born on October 28, 1955, is an American businessman and philanthropist, best known for co-founding Microsoft in 1975 with Paul Allen. He became a billionaire in 1987 and was ranked as the world's wealthiest person for 18 out of 24 years between 1995 and 2017. Gates briefly became the first centibillionaire in 1999, and as of May 2025, his net worth is estimated at $115.1 billion, making him the thirteenth-richest individual globally.\n",
      "\n",
      "Gates was educated at Lakeside School in Seattle, where he developed an interest in computing, and later enrolled at Harvard College. He dropped out in 1975 to focus on Microsoft, serving as CEO for 25 years, then transitioning to chief software architect until 2008. He stepped down as chairman in 2014 but continues to serve as a technology adviser. Gates resigned from the Microsoft board in 2020.\n"
     ]
    }
   ],
   "source": [
    "summary = await kernel.invoke(summarize, input=input_text)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ChatCompletion for Semantic Plugins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use chat completion models for creating plugins. Normally you would have to tweak the API to accommodate for a system and user role, but SK abstracts that away for you by using `kernel.add_service` and `AzureChatCompletion`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how to write an inline Semantic Function that gives a TLDR for a piece of text using a ChatCompletion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        deployment_name=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "\t\t\tendpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "\t\t\tapi_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "\t\t\tapi_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Nature and connection inspire growth.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "\n",
    "prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "IN **FIVE WORDS OR FEWER**, GIVE A TL;DR OF THE TEXT BELOW.\n",
    "\n",
    "RULES\n",
    "- ≤ 5 total words (count them!)\n",
    "- No direct quotes from the source\n",
    "- Plain, neutral wording\n",
    "- Avoid emojis, slang, or punctuation other than periods if essential\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "    1) A sunrise reminds us that each day begins with limitless potential waiting to be shaped.\n",
    "\n",
    "    2) Laughter shared between strangers can dissolve the weight of an entire afternoon.\n",
    "\n",
    "    3) A single seed, buried in silence, can one day split rock with the persistence of its roots.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "tldr_function = kernel.add_function(\n",
    "    function_name=\"tldrFunction\",\n",
    "    plugin_name=\"tldrPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "summary = await kernel.invoke(tldr_function, input=text)\n",
    "\n",
    "print(f\"Output: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Chat Experience with Kernel Arguments\n",
    "\n",
    "This example demonstrates how to create a basic chatbot by passing and updating kernel arguments with each user interaction.\n",
    "\n",
    "We introduce the Kernel Arguments object, which acts as a key-value store for data you provide to the kernel during execution.\n",
    "\n",
    "Here, chat history is stored locally in memory and will not persist beyond this Jupyter session.\n",
    "\n",
    "In later examples, we'll cover how to save chat history to disk for use in your own applications.\n",
    "\n",
    "As you converse with the bot, the chat context accumulates the conversation history. Each time the kernel runs, it uses the current kernel arguments and chat history to inform the AI's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have laid the foundation which will allow us to store an arbitrary amount of data in an external Vector Store above and beyond what could fit in memory at the expense of a little more latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a prompt outlining a dialogue chat bot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register your semantic function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"chat\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"user_input\", description=\"The user input\", is_required=True),\n",
    "        InputVariable(name=\"history\", description=\"The conversation history\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    function_name=\"chat\",\n",
    "    plugin_name=\"chatPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_system_message(\"You are a helpful chatbot who is good about giving song recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Kernel Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "arguments = KernelArguments(user_input=\"Hi, I'm looking for song suggestions\", history=str(chat_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat with the Bot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! What kind of music are you in the mood for? Any specific genres or artists you like?\n"
     ]
    }
   ],
   "source": [
    "response = await kernel.invoke(chat_function, arguments)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the history with the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_assistant_message(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep Chatting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(input_text: str) -> None:\n",
    "    # Save new message in the context variables\n",
    "    print(f\"User: {input_text}\")\n",
    "\n",
    "    # Process the user message and get an answer\n",
    "    answer = await kernel.invoke(chat_function, KernelArguments(user_input=input_text, history=chat_history))\n",
    "\n",
    "    # Show the response\n",
    "    print(f\"ChatBot: {answer}\")\n",
    "\n",
    "    chat_history.add_user_message(input_text)\n",
    "    chat_history.add_assistant_message(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function failed. Error: Argument 'history' has a value that doesn't support automatic encoding. Set allow_dangerously_set_content to 'True' for this argument and implement custom encoding, or provide the value as a string.\n",
      "Something went wrong in function invocation. During function invocation: 'chatPlugin-chat'. Error description: 'Argument 'history' has a value that doesn't support automatic encoding. Set allow_dangerously_set_content to 'True' for this argument and implement custom encoding, or provide the value as a string.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I love rock and metallica, I'd like to listen to metallica, any suggestion?\n"
     ]
    },
    {
     "ename": "KernelInvokeException",
     "evalue": "Error occurred while invoking function: 'chatPlugin-chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\kernel.py:202\u001b[39m, in \u001b[36mKernel.invoke\u001b[39m\u001b[34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m function.invoke(kernel=\u001b[38;5;28mself\u001b[39m, arguments=arguments, metadata=metadata)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationCancelledException \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:293\u001b[39m, in \u001b[36mKernelFunction.invoke\u001b[39m\u001b[34m(self, kernel, arguments, metadata, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_exception(current_span, e, attributes)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:278\u001b[39m, in \u001b[36mKernelFunction.invoke\u001b[39m\u001b[34m(self, kernel, arguments, metadata, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m stack = kernel.construct_call_stack(\n\u001b[32m    275\u001b[39m     filter_type=FilterTypes.FUNCTION_INVOCATION,\n\u001b[32m    276\u001b[39m     inner_function=\u001b[38;5;28mself\u001b[39m._invoke_internal,\n\u001b[32m    277\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[32m    280\u001b[39m KernelFunctionLogMessages.log_function_invoked_success(logger, \u001b[38;5;28mself\u001b[39m.fully_qualified_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:172\u001b[39m, in \u001b[36mKernelFunctionFromPrompt._invoke_internal\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invokes the function with the given arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m prompt_render_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._render_prompt(context)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompt_render_result.function_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:285\u001b[39m, in \u001b[36mKernelFunctionFromPrompt._render_prompt\u001b[39m\u001b[34m(self, context, is_streaming)\u001b[39m\n\u001b[32m    281\u001b[39m stack = context.kernel.construct_call_stack(\n\u001b[32m    282\u001b[39m     filter_type=FilterTypes.PROMPT_RENDERING,\n\u001b[32m    283\u001b[39m     inner_function=\u001b[38;5;28mself\u001b[39m._inner_render_prompt,\n\u001b[32m    284\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stack(prompt_render_context)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompt_render_context.rendered_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:303\u001b[39m, in \u001b[36mKernelFunctionFromPrompt._inner_render_prompt\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Render the prompt using the prompt template.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m context.rendered_prompt = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompt_template.render(context.kernel, context.arguments)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\prompt_template\\kernel_prompt_template.py:94\u001b[39m, in \u001b[36mKernelPromptTemplate.render\u001b[39m\u001b[34m(self, kernel, arguments)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Render the prompt template.\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m     82\u001b[39m \u001b[33;03mUsing the prompt template, replace the variables with their values\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m \n\u001b[32m     93\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_blocks(\u001b[38;5;28mself\u001b[39m._blocks, kernel, arguments)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\prompt_template\\kernel_prompt_template.py:115\u001b[39m, in \u001b[36mKernelPromptTemplate.render_blocks\u001b[39m\u001b[34m(self, blocks, kernel, arguments)\u001b[39m\n\u001b[32m    114\u001b[39m rendered_blocks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m arguments = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_trusted_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mKernelArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m allow_unsafe_function_output = \u001b[38;5;28mself\u001b[39m._get_allow_dangerously_set_function_output()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\prompt_template\\prompt_template_base.py:47\u001b[39m, in \u001b[36mPromptTemplateBase._get_trusted_arguments\u001b[39m\u001b[34m(self, arguments)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m arguments.items():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     new_args[name] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_encoded_value_or_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_args\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\prompt_template\\prompt_template_base.py:90\u001b[39m, in \u001b[36mPromptTemplateBase._get_encoded_value_or_default\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# For complex types, throw an exception if dangerous content is not allowed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m     91\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has a value that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt support automatic encoding. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSet allow_dangerously_set_content to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrue\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for this argument and implement custom encoding, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mor provide the value as a string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Argument 'history' has a value that doesn't support automatic encoding. Set allow_dangerously_set_content to 'True' for this argument and implement custom encoding, or provide the value as a string.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKernelInvokeException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m chat(\u001b[33m\"\u001b[39m\u001b[33mI love rock and metallica, I\u001b[39m\u001b[33m'\u001b[39m\u001b[33md like to listen to metallica, any suggestion?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mchat\u001b[39m\u001b[34m(input_text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Process the user message and get an answer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m answer = \u001b[38;5;28;01mawait\u001b[39;00m kernel.invoke(chat_function, KernelArguments(user_input=input_text, history=chat_history))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Show the response\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChatBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\siddh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\semantic_kernel\\kernel.py:211\u001b[39m, in \u001b[36mKernel.invoke\u001b[39m\u001b[34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    207\u001b[39m     logger.error(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSomething went wrong in function invocation. During function invocation:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.fully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Error description: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[32m    212\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError occurred while invoking function: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.fully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mKernelInvokeException\u001b[39m: Error occurred while invoking function: 'chatPlugin-chat'"
     ]
    }
   ],
   "source": [
    "await chat(\"I love rock and metallica, I'd like to listen to metallica, any suggestion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: that sounds interesting, what is it about?\n",
      "ChatBot: Metallica's music often explores themes such as personal struggle, war, and societal issues. For example, \"One\" is a powerful anti-war song that tells the story of a soldier who has been severely injured and longs for freedom. \"Master of Puppets\" delves into addiction and the loss of control it brings. Their sound is characterized by heavy guitar riffs, fast tempos, and dynamic song structures, making them a defining band in the thrash metal genre. If you're interested in the lyrics or themes of specific songs, I can provide more details!\n"
     ]
    }
   ],
   "source": [
    "await chat(\"that sounds interesting, what is it about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: if I listen that song, what exactly will I feel?\n",
      "ChatBot: Listening to Metallica's music can evoke a range of emotions. For example:\n",
      "\n",
      "- **\"One\"**: You might feel a deep sense of empathy and sadness as it portrays the struggles of a soldier. The haunting melodies and intense lyrics can create a somber yet powerful atmosphere.\n",
      "\n",
      "- **\"Master of Puppets\"**: This song often instills feelings of urgency and intensity. The fast-paced riffs and aggressive vocals can give you a rush, while the lyrics may provoke thoughts about the dangers of addiction and manipulation.\n",
      "\n",
      "- **\"Enter Sandman\"**: This track can create a sense of tension and thrill. The eerie opening and driving beat can evoke feelings of excitement mixed with a bit of fear, as it touches on themes of nightmares and childhood fears.\n",
      "\n",
      "Each song has its own unique vibe, so you might experience a mix of adrenaline, introspection, or catharsis depending on the track. Let me know if you want to dive deeper into any specific song!\n"
     ]
    }
   ],
   "source": [
    "await chat(\"if I listen that song, what exactly will I feel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: could you list some more songs I could listen to?\n",
      "ChatBot: Absolutely! Here are some more Metallica songs you should check out:\n",
      "\n",
      "1. **\"The Unforgiven\"** - A powerful ballad about struggle and regret.\n",
      "2. **\"Seek & Destroy\"** - A classic that captures the raw energy of their early sound.\n",
      "3. **\"Creeping Death\"** - A fan favorite that combines heavy riffs with epic storytelling.\n",
      "4. **\"Nothing Else Matters\"** - A beautiful, introspective track that showcases their softer side.\n",
      "5. **\"For Whom the Bell Tolls\"** - Known for its iconic intro and themes of mortality.\n",
      "6. **\"Sad But True\"** - A heavy, groove-laden track that explores self-reflection.\n",
      "7. **\"The Day That Never Comes\"** - A more modern take with a mix of melody and aggression.\n",
      "8. **\"Battery\"** - A fast-paced opener from the \"Master of Puppets\" album that sets a high-energy tone.\n",
      "\n",
      "These songs offer a great mix of their styles and themes. Enjoy your listening!\n"
     ]
    }
   ],
   "source": [
    "await chat(\"could you list some more songs I could listen to?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After chatting for a while, we have built a growing history, which we are attaching to each prompt and which contains the full conversation. Let's take a look!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chat_history><message role=\"system\"><text>You are a helpful chatbot who is good about giving song recommendations.</text></message><message role=\"assistant\"><text>Sure! What kind of mood are you in or what genre of music do you like?</text></message><message role=\"user\"><text>I love rock and metallica, I'd like to listen to metallica, any suggestion?</text></message><message role=\"assistant\"><text>If you love Metallica, you should definitely check out their iconic albums like \"Master of Puppets\" and \"Ride the Lightning.\" Some standout tracks to listen to are \"Enter Sandman,\" \"One,\" and \"Fade to Black.\" If you’re looking for something a bit different but still in the same vein, you might enjoy bands like Megadeth, Slayer, or Pantera. Let me know if you want more specific recommendations!</text></message><message role=\"user\"><text>that sounds interesting, what is it about?</text></message><message role=\"assistant\"><text>Metallica's music often explores themes such as personal struggle, war, and societal issues. For example, \"One\" is a powerful anti-war song that tells the story of a soldier who has been severely injured and longs for freedom. \"Master of Puppets\" delves into addiction and the loss of control it brings. Their sound is characterized by heavy guitar riffs, fast tempos, and dynamic song structures, making them a defining band in the thrash metal genre. If you're interested in the lyrics or themes of specific songs, I can provide more details!</text></message><message role=\"user\"><text>if I listen that song, what exactly will I feel?</text></message><message role=\"assistant\"><text>Listening to Metallica's music can evoke a range of emotions. For example:\n",
      "\n",
      "- **\"One\"**: You might feel a deep sense of empathy and sadness as it portrays the struggles of a soldier. The haunting melodies and intense lyrics can create a somber yet powerful atmosphere.\n",
      "\n",
      "- **\"Master of Puppets\"**: This song often instills feelings of urgency and intensity. The fast-paced riffs and aggressive vocals can give you a rush, while the lyrics may provoke thoughts about the dangers of addiction and manipulation.\n",
      "\n",
      "- **\"Enter Sandman\"**: This track can create a sense of tension and thrill. The eerie opening and driving beat can evoke feelings of excitement mixed with a bit of fear, as it touches on themes of nightmares and childhood fears.\n",
      "\n",
      "Each song has its own unique vibe, so you might experience a mix of adrenaline, introspection, or catharsis depending on the track. Let me know if you want to dive deeper into any specific song!</text></message><message role=\"user\"><text>could you list some more songs I could listen to?</text></message><message role=\"assistant\"><text>Absolutely! Here are some more Metallica songs you should check out:\n",
      "\n",
      "1. **\"The Unforgiven\"** - A powerful ballad about struggle and regret.\n",
      "2. **\"Seek &amp; Destroy\"** - A classic that captures the raw energy of their early sound.\n",
      "3. **\"Creeping Death\"** - A fan favorite that combines heavy riffs with epic storytelling.\n",
      "4. **\"Nothing Else Matters\"** - A beautiful, introspective track that showcases their softer side.\n",
      "5. **\"For Whom the Bell Tolls\"** - Known for its iconic intro and themes of mortality.\n",
      "6. **\"Sad But True\"** - A heavy, groove-laden track that explores self-reflection.\n",
      "7. **\"The Day That Never Comes\"** - A more modern take with a mix of melody and aggression.\n",
      "8. **\"Battery\"** - A fast-paced opener from the \"Master of Puppets\" album that sets a high-energy tone.\n",
      "\n",
      "These songs offer a great mix of their styles and themes. Enjoy your listening!</text></message></chat_history>\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Native Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we learned how to execute semantic functions inline and how to run prompts from a file.\n",
    "\n",
    "In this section, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "- Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "- Using external data sources to gather data to concatenate into your prompt.\n",
    "- Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 100 and a user input as the upper limit. We'll use this number to create 100-x paragraphs of text when passed to a semantic function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create our native function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 100-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between 100-x\",\n",
    "        name=\"GenerateNumberHundredOrHigher\",\n",
    "    )\n",
    "    def generate_number_hundred_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 10-<input>\n",
    "        Example:\n",
    "            \"102\" => rand(100,102)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(100, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two engineers on an adventure. `$input` is a default variable semantic functions can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a short story about two engineers on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long. It must be this length.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"story\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "engineer_story = kernel.add_function(\n",
    "    function_name=\"EngineerStory\",\n",
    "    plugin_name=\"EngineerPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.add_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberHundredOrHigher\"]\n",
    "number_result = await generate_number_three_or_higher(kernel, input=101)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await engineer_story.invoke(kernel, input=number_result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: depending on which model you're using, it may not respond with the proper number of paragraphs. Please ensure to stop the cell after running for a while, since it will continue generating n (>100) paragraphs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating an engineer story exactly 100 paragraphs long.\n",
      "=====================================================\n",
      "**Title: The Engineers' Quest**\n",
      "\n",
      "**Paragraph 1:**\n",
      "In a small town nestled between rolling hills and sparkling streams, two engineers named Alex and Sam had a dream. They were passionate about building things that could help their community and the world at large.\n",
      "\n",
      "**Paragraph 2:**\n",
      "One sunny morning, they gathered in their workshop, a cozy space filled with tools, blueprints, and the scent of fresh wood. Their eyes sparkled with excitement as they spoke about their next big project: a solar-powered water purifier.\n",
      "\n",
      "**Paragraph 3:**\n",
      "“Imagine clean water for everyone,” Sam exclaimed, sketching ideas on a whiteboard. “This could change lives!” Alex nodded in agreement, their shared enthusiasm fueling their creativity.\n",
      "\n",
      "**Paragraph 4:**\n",
      "After hours of brainstorming, they decided to take a weekend to prototype their invention. They packed their bags with tools, snacks, and a bright yellow tent, ready for an adventure in the nearby forest.\n",
      "\n",
      "**Paragraph 5:**\n",
      "As they hiked through the lush greenery, they discussed the challenges they might face. “We’ll need to find a reliable water source,” Alex said, glancing at the map. “And we’ll have to test our design thoroughly.”\n",
      "\n",
      "**Paragraph 6:**\n",
      "“Let’s make it a fun challenge,” Sam suggested, smiling. “We can turn it into a race against time! We’ll see who can find the best spot first!” With laughter, they both picked up their pace.\n",
      "\n",
      "**Paragraph 7:**\n",
      "After an hour of walking, they stumbled upon a crystal-clear stream that flowed gently over smooth rocks. “This is perfect!” Sam shouted, pointing excitedly. Alex agreed, their hearts racing with possibilities.\n",
      "\n",
      "**Paragraph 8:**\n",
      "Setting up their tent nearby, they began assembling the components of their water purifier. They worked side by side, each contributing unique skills and ideas, blending their talents seamlessly.\n",
      "\n",
      "**Paragraph 9:**\n",
      "As dusk fell, they lit a small campfire and roasted marshmallows. The stars twinkled above, and they shared stories of their childhood dreams, laughter echoing through the trees.\n",
      "\n",
      "**Paragraph 10:**\n",
      "The next morning, they woke up early, energized and ready to dive into their project. “Let’s test it out!” Alex said, eyes gleaming with determination. They gathered materials from around them.\n",
      "\n",
      "**Paragraph 11:**\n",
      "The duo meticulously constructed the purifier, integrating solar panels and a series of filters they had designed. Each step brought them closer to their goal, but also posed new challenges.\n",
      "\n",
      "**Paragraph 12:**\n",
      "“Uh-oh, I think we need more sunlight,” Sam said, noticing the shadows creeping over their work. Alex quickly adjusted the angle of the solar panels, ensuring they captured the sun’s rays.\n",
      "\n",
      "**Paragraph 13:**\n",
      "After hours of hard work, the purifier was complete. They filled it with water from the stream and eagerly awaited the results. “Let’s hope this works!” Alex said, crossing their fingers.\n",
      "\n",
      "**Paragraph 14:**\n",
      "Minutes felt like hours, but finally, the water began to flow through the filters. “Look! It’s clear!” Sam shouted, astonished. They both cheered, their excitement bubbling over like the water itself.\n",
      "\n",
      "**Paragraph 15:**\n",
      "With their prototype working, they decided to take it to the nearby village. “We can show them how it works and get feedback,” Alex suggested, a sense of purpose guiding their steps.\n",
      "\n",
      "**Paragraph 16:**\n",
      "As they approached the village, they were greeted with curious smiles and friendly waves. The villagers gathered around, eager to see what the engineers had brought.\n",
      "\n",
      "**Paragraph 17:**\n",
      "“Hello, everyone! We’ve created a solar-powered water purifier!” Sam announced, holding up their invention. The crowd gasped, intrigued by the shimmering panels and clean design.\n",
      "\n",
      "**Paragraph 18:**\n",
      "After a brief explanation of how it worked, they demonstrated the purifier using water from a nearby well. The villagers watched in awe as the once murky water transformed into something pure and clear.\n",
      "\n",
      "**Paragraph 19:**\n",
      "“Thank you so much!” an elderly woman said, tears in her eyes. “This will change our lives.” Alex and Sam smiled, knowing that their hard work had made a difference.\n",
      "\n",
      "**Paragraph 20:**\n",
      "The villagers asked questions, eager to learn more about the technology. Sam and Alex patiently explained every detail, excited to share their knowledge and inspire others.\n",
      "\n",
      "**Paragraph 21:**\n",
      "As night fell, the village held a celebration in honor of the engineers. Music filled the air, and laughter echoed through the streets as people danced and shared food.\n",
      "\n",
      "**Paragraph 22:**\n",
      "Alex and Sam felt a deep sense of accomplishment, knowing their invention could help so many. They realized the importance of collaboration and community in their work.\n",
      "\n",
      "**Paragraph 23:**\n",
      "The next day, they received requests for more purifiers from neighboring villages. “We should create a plan to help them all,” Alex said, already thinking of ways to expand their project.\n",
      "\n",
      "**Paragraph 24:**\n",
      "Sam agreed, adding, “Let’s also teach them how to build their own. That way, they can be self-sufficient!” Their hearts swelled with pride at the thought of sharing their skills.\n",
      "\n",
      "**Paragraph 25:**\n",
      "They spent the next few weeks traveling between villages, teaching workshops and building more purifiers. Each community welcomed them with open arms, and they formed friendships that blossomed.\n",
      "\n",
      "**Paragraph 26:**\n",
      "As they traveled, they learned about the unique challenges each village faced. They adapted their designs, incorporating local materials and knowledge into their solutions.\n",
      "\n",
      "**Paragraph 27:**\n",
      "One afternoon, they visited a village that relied on a nearby river for water. “They need something that can handle larger volumes,” Sam noted, sketching ideas in the dirt.\n",
      "\n",
      "**Paragraph 28:**\n",
      "Together, they brainstormed a new design, one that could purify bigger amounts of water quickly. “This is what engineering is all about,” Alex said, feeling inspired by the challenge.\n",
      "\n",
      "**Paragraph 29:**\n",
      "With the villagers’ help, they built a larger prototype. The sense of teamwork was invigorating, as everyone collaborated to create something meaningful.\n",
      "\n",
      "**Paragraph 30:**\n",
      "When they tested the new purifier, it worked flawlessly. The joy on the villagers’ faces made every challenge worth it. “We did it together!” Sam exclaimed, embracing the villagers.\n",
      "\n",
      "**Paragraph 31:**\n",
      "As they continued their journey, Alex and Sam realized that their adventure was about more than just engineering. It was about connecting with people and building a better future together.\n",
      "\n",
      "**Paragraph 32:**\n",
      "They returned to their hometown, their hearts full of stories and lessons learned. The experience had transformed them, inspiring new ideas for future projects.\n",
      "\n",
      "**Paragraph 33:**\n",
      "Back in their workshop, they started sketching plans for a community center that would serve as a hub for learning and innovation. “We can host workshops, just like we did!” Alex suggested.\n",
      "\n",
      "**Paragraph 34:**\n",
      "Sam’s eyes sparkled. “And we can create a library of resources for everyone! This can be a place where ideas flow freely, and everyone can contribute.”\n",
      "\n",
      "**Paragraph 35:**\n",
      "With renewed energy, they dedicated themselves to making the community center a reality. They organized fundraising events and rallied support from friends, family, and local businesses.\n",
      "\n",
      "**Paragraph 36:**\n",
      "The community responded enthusiastically, donating time, materials, and funds. It felt as if everyone wanted to be a part of something bigger, and Alex and Sam were at the heart of it.\n",
      "\n",
      "**Paragraph 37:**\n",
      "As the center began to take shape, they planned a grand opening. They wanted to celebrate not just the building but the spirit of collaboration and creativity that had blossomed.\n",
      "\n",
      "**Paragraph 38:**\n",
      "On the day of the opening, the sun shone brightly, and the air buzzed with excitement. People from all over the town gathered, eager to see the new space and connect with each other.\n",
      "\n",
      "**Paragraph 39:**\n",
      "Alex and Sam spoke about their journey, the adventures they had, and the importance of working together. “When we pool our talents, we can achieve incredible things,” Sam said passionately.\n",
      "\n",
      "**Paragraph 40:**\n",
      "As the crowd cheered, they cut the ribbon, and the community center was officially open. Inside, the walls were adorned with artwork created by local children, representing dreams and aspirations.\n",
      "\n",
      "**Paragraph 41:**\n",
      "Workshops began immediately, with villagers teaching each other skills like sewing, woodworking, and gardening. The center became a hub of creativity and collaboration.\n",
      "\n",
      "**Paragraph 42:**\n",
      "Alex and Sam were overjoyed to see their vision come to life. They spent their days at the center, sharing ideas and learning from others. Every corner was filled with laughter and inspiration.\n",
      "\n",
      "**Paragraph 43:**\n",
      "One afternoon, an elderly man approached them with a proposal. “I’ve been thinking about a wind turbine to supplement our energy needs,” he said. “Could you help us?”\n",
      "\n",
      "**Paragraph 44:**\n",
      "“Absolutely!” Alex replied, thrilled at the prospect of another project. They set to work, designing a wind turbine that would harness the power of the wind.\n",
      "\n",
      "**Paragraph 45:**\n",
      "As they collaborated with the villagers, they discovered innovative ways to use recycled materials in their design. The project became a true community effort, showcasing everyone’s strengths.\n",
      "\n",
      "**Paragraph 46:**\n",
      "With their combined knowledge, they built the wind turbine. Watching it spin in the breeze felt like a symbol of their collective achievements and shared dreams.\n",
      "\n",
      "**Paragraph 47:**\n",
      "The day they turned it on was filled with anticipation. The villagers gathered, holding their breath as the turbine began to generate electricity. Cheers erupted, echoing through the hills.\n",
      "\n",
      "**Paragraph 48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating an engineer story exactly {number_result.value} paragraphs long.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Functions with Annotated Parameters\n",
    "\n",
    "That works! But let's expand on our example to make it more generic.\n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the Python's `Annotated` class to hold these variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're add the `@kernel_function` decorator that holds the name of the function as well as an optional description. The input parameters are configured as part of the function's signature, and we use the `Annotated` type to specify the required input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Annotated\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"GenerateNumber\",\n",
    "        description=\"Generate a random number between min and max\",\n",
    "    )\n",
    "    def generate_number(\n",
    "        self,\n",
    "        min: Annotated[int, \"the minimum number of paragraphs\"],\n",
    "        max: Annotated[int, \"the maximum number of paragraphs\"] = 10,\n",
    "    ) -> Annotated[int, \"the output is a number\"]:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(min, max))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {min} and {max}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number_plugin = kernel.add_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our EngineerStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two engineers on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-35-turbo\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "engineer_story = kernel.add_function(\n",
    "    function_name=\"EngineerStory\",\n",
    "    plugin_name=\"EngineerPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a paragraph count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a engineer story 3 paragraphs long.\n"
     ]
    }
   ],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value\n",
    "print(f\"Generating a engineer story {num_paragraphs} paragraphs long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke our engineer_story function using the `kernel` and the keyword arguments `paragraph_count` and `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the output to the semantic story function\n",
    "desired_language = \"Spanish\"\n",
    "story = await engineer_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating an engineer story 3 paragraphs long in Spanish.\n",
      "=====================================================\n",
      "Era un soleado día de primavera cuando dos Corgis, Toby y Luna, decidieron aventurarse más allá de su jardín. Siempre habían soñado con explorar el mundo más allá de las cercas, así que, con un salto de emoción, cruzaron la puerta y se encontraron en el parque cercano. El aroma de las flores y el sonido de los pájaros les llenaron de alegría. Juntos, se prometieron que hoy sería un día especial y lleno de sorpresas.\n",
      "\n",
      "Mientras exploraban, Toby y Luna se toparon con un grupo de animales que jugaban a la pelota. Al principio, se sintieron un poco tímidos, pero pronto, la curiosidad superó su miedo. Se acercaron y, con un ladrido amistoso, pidieron unirse al juego. Todos los animales aceptaron con gusto, y así, los dos Corgis pasaron horas corriendo, saltando y riendo en compañía de sus nuevos amigos. Toby y Luna aprendieron que, a veces, lo desconocido puede traer momentos maravillosos si se tiene el valor de acercarse.\n",
      "\n",
      "Al caer la tarde, los Corgis regresaron a casa, cansados pero felices. Habían descubierto que la amistad y la diversión se encuentran en los lugares más inesperados. Con una sonrisa en sus rostros, decidieron que todos los días deberían ser una aventura. Desde entonces, Toby y Luna se comprometieron a explorar nuevos horizontes y hacer amigos en cada rincón, recordando siempre que la verdadera felicidad se comparte.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating an engineer story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Prompt Functions!\n",
    "\n",
    "We will make our EngineerStory semantic function call a native function `GenerateNames` which will return names for our characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @kernel_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @kernel_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Ada\", \"Grace\", \"Linus\", \"Alan\", \"Margaret\", \"Dennis\", \"Barbara\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.add_plugin(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a short story about two engineers on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=service_id,\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"engineer-new\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "engineer_story = kernel.add_function(\n",
    "    function_name=\"EngineerStoryUpdated\",\n",
    "    plugin_name=\"EngineerPluginUpdated\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_language = \"French\"\n",
    "story = await engineer_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating an engineer story 1 paragraphs long in French.\n",
      "=====================================================\n",
      "Un jour ensoleillé, deux ingénieurs passionnés, Clara et Lucas, décidèrent d'emmener leurs adorables corgis, Pizza et Boots, en randonnée dans la forêt voisine pour tester leur nouveau drone. En survolant les arbres, ils découvrirent un vieux pont en bois qui semblait prêt à s'effondrer. Au lieu de l'ignorer, ils décidèrent de le réparer ensemble, utilisant leurs compétences en ingénierie et en travail d'équipe. Grâce à leur détermination et à l'aide de Pizza et Boots, qui aboyaient joyeusement, ils restaurèrent le pont, permettant à d'autres randonneurs de traverser en toute sécurité. Cette aventure leur rappela que la collaboration et la curiosité peuvent transformer des défis en opportunités, tout en renforçant les liens d'amitié et en rendant le monde un peu meilleur.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating an engineer story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and prompt functions and register them to the kernel\n",
    "- We've seen how we can use Kernel Arguments to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within a prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groundedness Checking Plugins\n",
    "\n",
    "Large language models (LLMs) are known to sometimes generate information that isn't supported by their input—often referred to as \"hallucinations\" or, more precisely, \"ungrounded additions.\" These are details in the output that cannot be directly verified. To determine if something in an LLM's response is accurate, we can either check if it appears in the provided prompt (\"narrow grounding\") or rely on general world knowledge (\"broad grounding\").\n",
    "\n",
    "In this section, we'll implement a basic grounding pipeline to identify and address ungrounded additions in summary texts compared to their original sources. The process involves three main steps:\n",
    "\n",
    "1. Extract a list of entities from the summary text.\n",
    "2. Check whether these entities are present in the original (grounding) text.\n",
    "3. Remove any entities from the summary that are not grounded in the original text.\n",
    "\n",
    "Here, an \"entity\" refers to a named object, such as a person or place (e.g., \"Dean\" or \"Seattle\"). While entities can also include claims that connect concepts (like \"Dean lives near Seattle\"), this section will focus on the simpler case of named objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define our grounding text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_text = \"\"\"I was born in the rolling hills of Alsace, into a family whose name carried both honor and modesty in equal measure. For generations, our kin had stewarded the vineyards of the region, tending the earth with patience and a quiet devotion. My father, when not in the fields, could be found at the communal table in Colmar, dispensing wisdom earned through years of measured hardship and principled living. Though he held no titles, his reputation as a judicious man was universally acknowledged.\n",
    "\n",
    "In the autumn of his life, he made a decision that would redefine us. A neighbor, once renowned for his craftsmanship, had fallen upon misfortune—his hands, once steady and exacting, now trembled with age and wear. He withdrew from public life, ashamed that he could no longer fashion vessels of beauty and use. My father, moved by compassion and respect, insisted on restoring the craftsman’s dignity. He offered him work in our cooperage—not as a mere hand, but as a teacher to the younger apprentices, whose eagerness matched only their inexperience.\n",
    "\n",
    "At first, the neighbor resisted, his pride stubborn like the ancient oaks. Yet, day by day, he lent his expertise to the apprentices, shaping their skill as deftly as he once shaped oak staves. The workshop, once just a place of labor, transformed into a hall of shared stories—echoed with laughter, hushed regret, and cautious hope. In time, the man regained a portion of his old strength, not in the flexibility of his joints, but in the steady pride he carried once more.\n",
    "\n",
    "When he passed in winter’s depth, it was not grief alone that filled the room, but a sense of quiet triumph: of pride reclaimed, skill passed forward, and a life tenderly redeemed by community and care.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Plugins\n",
    "\n",
    "We are going to be using the grounding plugin, to check its quality, and remove ungrounded additions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this is an official Microsoft plugin taken from the semantic-kernel repository. You can use it as a reference for your own plugins.\n",
    "plugins_directory = \"./plugins\"\n",
    "\n",
    "groundingSemanticFunctions = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"GroundingPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract the individual semantic functions for our use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extraction = groundingSemanticFunctions[\"ExtractEntities\"]\n",
    "reference_check = groundingSemanticFunctions[\"ReferenceCheckEntities\"]\n",
    "entity_excision = groundingSemanticFunctions[\"ExciseEntities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Individual Semantic Functions\n",
    "\n",
    "We will start by calling the individual grounding functions in turn, to show their use. For this we need to create a same summary text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My father, a respected Genevese statesman, was devoted to his friend Beaufort, a merchant who—ruined by misfortune—had withdrawn in poverty to Lucerne. Troubled by Beaufort’s plight, my father searched until he found him in a mean street near the Reuss. Beaufort had saved only a small remnant of his fortune, insufficient to support himself and his steadfast daughter, Caroline. Caroline eked out a living with plain work and straw-plaiting, but within ten months Beaufort died, leaving her penniless. My father placed her under his relatives’ care in Geneva and, two years later, took her as his wife. \n"
     ]
    }
   ],
   "source": [
    "summary_text = \"\"\"\n",
    "My father, a respected Genevese statesman, was devoted to his friend Beaufort, a merchant who—ruined by misfortune—had withdrawn in poverty to Lucerne. Troubled by Beaufort’s plight, my father searched until he found him in a mean street near the Reuss. Beaufort had saved only a small remnant of his fortune, insufficient to support himself and his steadfast daughter, Caroline. Caroline eked out a living with plain work and straw-plaiting, but within ten months Beaufort died, leaving her penniless. My father placed her under his relatives’ care in Geneva and, two years later, took her as his wife.\n",
    "\"\"\"\n",
    "\n",
    "summary_text = summary_text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grounding plugin operates in three steps:\n",
    "\n",
    "1. Identify entities within the summary text.\n",
    "2. Check if these entities are present in the grounding text.\n",
    "3. Remove any entities from the summary that are not supported by the grounding text.\n",
    "\n",
    "Let's proceed to call each semantic function individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Entities\n",
    "\n",
    "The first function we need is entity extraction. We are going to take our summary text, and get a list of entities found within it. For this we use `entity_extraction()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<entities>\n",
      "- Genevese: Refers to a person from Geneva, Switzerland, indicating a place of origin.\n",
      "- Beaufort: The name of a merchant and a friend of the narrator's father.\n",
      "- Lucerne: A city in Switzerland where Beaufort had withdrawn.\n",
      "- Reuss: A river in Switzerland, near which Beaufort was found.\n",
      "- Caroline: The name of Beaufort's steadfast daughter.\n",
      "- Geneva: A city in Switzerland where Caroline was placed under the care of her relatives.\n",
      "</entities>\n"
     ]
    }
   ],
   "source": [
    "extraction_result = await kernel.invoke(\n",
    "    entity_extraction,\n",
    "    input=summary_text,\n",
    "    topic=\"people and places\",\n",
    "    example_entities=\"John, Jane, mother, brother, Paris, Rome\",\n",
    ")\n",
    "\n",
    "print(extraction_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have our list of entities in the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the reference check\n",
    "\n",
    "We now use the grounding text to see if the entities we found are grounded. We start by adding the grounding text to our context:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, we can run the reference checking function. This will use both the entity list in the input, and the `reference_context` in the context object itself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ungrounded_entities>\n",
      "- Genevese\n",
      "- Beaufort\n",
      "- Lucerne\n",
      "- Reuss\n",
      "- Caroline\n",
      "- Geneva\n",
      "</ungrounded_entities>\n"
     ]
    }
   ],
   "source": [
    "grounding_result = await kernel.invoke(reference_check, input=extraction_result.value, reference_context=grounding_text)\n",
    "\n",
    "print(grounding_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excising the ungrounded entities\n",
    "\n",
    "Finally we can remove the ungrounded entities from the summary text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My father, a respected statesman, was devoted to his friend, a merchant who—ruined by misfortune—had withdrawn in poverty to a small town. Troubled by his plight, my father searched until he found him in a mean street near a river. He had saved only a small remnant of his fortune, insufficient to support himself and his steadfast daughter. She eked out a living with plain work and straw-plaiting, but within ten months he died, leaving her penniless. My father placed her under his relatives’ care and, two years later, took her as his wife.\n"
     ]
    }
   ],
   "source": [
    "excision_result = await kernel.invoke(entity_excision, input=summary_text, ungrounded_entities=grounding_result.value)\n",
    "\n",
    "print(excision_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Multiple Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will see how you can in a single request, have the LLM model return multiple results per prompt. This is useful for running experiments where you want to evaluate the robustness of your prompt and the parameters of your config against a particular large language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up the text and chat services we will be submitting prompts to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureChatPromptExecutionSettings,  # noqa: F401\n",
    "    AzureTextCompletion,\n",
    ")\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure Azure LLM service\n",
    "service_id = None\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "service_id = \"default\"\n",
    "aoai_chat_service = AzureChatCompletion(\n",
    "    service_id=\"aoai_chat\",\n",
    ")\n",
    "aoai_text_service = AzureTextCompletion(\n",
    "    service_id=\"aoai_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Azure OpenAI Chat Completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_oai_prompt_execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"aoai_chat\",\n",
    "    max_tokens=80,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: ...enjoy a healthy breakfast to kickstart my day! After that, I’ll take some time to set my goals for the day and prioritize my tasks. Maybe I’ll dive into a good book or listen to an inspiring podcast while I sip on my coffee. Later, I might meet up with friends for lunch or explore a new part of town. In the evening, I'll unwind with some yoga\n",
      "Result 2: ...enjoy a delicious breakfast to fuel my day. After that, I plan to tackle some tasks I've been putting off, like organizing my workspace and catching up on reading. In the afternoon, I might meet up with friends for lunch or explore a new spot in town. I'll make sure to take some time for myself too—maybe meditate or do some journaling. As the day winds down,\n",
      "Result 3: ...enjoy a delicious breakfast to fuel my day. After that, I plan to tackle my to-do list with energy and focus. Maybe I'll take some time to read a few chapters of a book I've been meaning to dive into. In the afternoon, I could meet up with friends for coffee or explore a local park. As the sun sets, I might unwind with some music or a movie,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "content = (\n",
    "    \"I am going to complete all my projects tomorrow. I will wake up, start working on them, build a plan, have coffee...\"\n",
    ")\n",
    "chat = ChatHistory()\n",
    "chat.add_user_message(content)\n",
    "results = await aoai_chat_service.get_chat_message_contents(\n",
    "    chat_history=chat, settings=az_oai_prompt_execution_settings\n",
    ")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}: {result!s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
